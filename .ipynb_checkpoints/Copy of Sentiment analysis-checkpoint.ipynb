{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hqjXnwj9S-0I"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\reby varghese\\anaconda3\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: joblib in c:\\users\\reby varghese\\anaconda3\\lib\\site-packages (from nltk) (0.16.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\reby varghese\\anaconda3\\lib\\site-packages (from nltk) (4.47.0)\n",
      "Requirement already satisfied: regex in c:\\users\\reby varghese\\anaconda3\\lib\\site-packages (from nltk) (2020.6.8)\n",
      "Requirement already satisfied: click in c:\\users\\reby varghese\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "j3S7HMdjS43k"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Reby\n",
      "[nltk_data]     Varghese\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Reby\n",
      "[nltk_data]     Varghese\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-4a9cae7a652b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m#cleaing text using regex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTransformerMixin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "#downloading stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import seaborn as sns\n",
    "nltk.download('wordnet')\n",
    "#cleaing text using regex\n",
    "import re\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hMP02fc8TWBn"
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv('TheSocialDilemma.csv')\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pjXImW01TWxd"
   },
   "outputs": [],
   "source": [
    "data=data[['text', 'Sentiment']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fGN1uBDwTtO0"
   },
   "outputs": [],
   "source": [
    "data.iloc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uEWXhfEWT9A9"
   },
   "outputs": [],
   "source": [
    "data['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "POujnIjP0Hat"
   },
   "outputs": [],
   "source": [
    "sns.countplot(data['Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DQZ-BRYsUt00"
   },
   "outputs": [],
   "source": [
    "data.text.values\n",
    "text_data=\" \".join(data.text.values)\n",
    "text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8D_qY9YcV5TN"
   },
   "outputs": [],
   "source": [
    "\n",
    "#downloading stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "#cleaing text using regex\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AjOLXlvR-d0d"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "  text=str(text)\n",
    "  text=text.lower()\n",
    "  text=text.strip()\n",
    "  text = [re.sub('[^a-zA-Z.]',\"\", Word) for Word in text.split()]\n",
    "  english_words=set(stopwords.words('english'))\n",
    "  word_stemmer=SnowballStemmer('english')\n",
    "  lemetizer=WordNetLemmatizer()\n",
    "\n",
    "  word=[Word for Word in text if Word not in english_words]\n",
    "  word=[word_stemmer.stem(Word) for Word in word]\n",
    "  word=[lemetizer.lemmatize(Word) for Word in word]\n",
    "\n",
    "  new_text=\" \".join(word)\n",
    "\n",
    "  return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UJfToii_pM9Y"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text = u'This is a smiley face \\U0001f602'\n",
    "print(text) # with emoji\n",
    "\n",
    "def deEmojify(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)\n",
    "\n",
    "print(deEmojify(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zSmMHthMvRof"
   },
   "outputs": [],
   "source": [
    "def display(df):\n",
    "  text = \" \".join(cat for cat in df['text'])\n",
    "  cleaned_text=clean_text(text)\n",
    "  occurence=Counter(cleaned_text.split())\n",
    "  print(occurence)\n",
    "\n",
    "  word_cloud = WordCloud(collocations = False, background_color = 'white').generate(cleaned_text)\n",
    "  plt.imshow(word_cloud)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HW9Qmm_tvuAd"
   },
   "outputs": [],
   "source": [
    "display(data[data['Sentiment']=='Positive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SV9elEPLwGas"
   },
   "outputs": [],
   "source": [
    "display(data[data['Sentiment']=='Negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VBGYB3kKwGdn"
   },
   "outputs": [],
   "source": [
    "display(data[data['Sentiment']=='Neutral'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DIOq-o3gpNRw"
   },
   "outputs": [],
   "source": [
    "class CleanEmojiTransformer(TransformerMixin):\n",
    "   def transform(self, X, **transform_params):\n",
    "      if type(X) == str :\n",
    "        return [deEmojify(X)]\n",
    "      else:\n",
    "        result= [deEmojify(text) for text in X]\n",
    "        return result\n",
    "   def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iez034OEKova"
   },
   "outputs": [],
   "source": [
    "class CleanTextTransformer(TransformerMixin):\n",
    "   def transform(self, X, **transform_params):\n",
    "      if type(X) == str :\n",
    "        return [clean_text(X)]\n",
    "      else:\n",
    "        result= [clean_text(text) for text in X]\n",
    "        return result\n",
    "   def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MUOCGgYBHgCI"
   },
   "outputs": [],
   "source": [
    "tar= {\n",
    "    'Positive' : 0,\n",
    "    'Neutral' : 1,\n",
    "    'Negative' : 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pi7x0twKQ2bi"
   },
   "outputs": [],
   "source": [
    "data['Target']=data['Sentiment'].map(tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oG5uAVx-IYKY"
   },
   "outputs": [],
   "source": [
    "data['Target'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rnt3n3L9KoyO"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(data['text'], data['Target'], test_size=0.3, random_state=10)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ibIbytTuRJ0"
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ccI6N4BYahaJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "model_1=SVC(kernel='linear')\n",
    "pipe_1 = Pipeline([('deEmojify', CleanEmojiTransformer()),\n",
    "                 ('cleaner', CleanTextTransformer()),\n",
    "                 ('vectorizer', vectorizer),\n",
    "                 ('model',model_1)])\n",
    "\n",
    "\n",
    "pipe_1.fit(x_train,y_train)\n",
    "pred_1=pipe_1.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rPzqpY4ZhIqZ"
   },
   "outputs": [],
   "source": [
    "pipe_1.predict(x_test[14628])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIjzL8qIn2C_"
   },
   "outputs": [],
   "source": [
    "pred_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lBvSEvHKoemh"
   },
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FZWAwc4sn2P2"
   },
   "outputs": [],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wZxlc91AHCTq"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(classification_report(y_test,pred_1))\n",
    "print(confusion_matrix(y_test,pred_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zDtmTXO7HCWO"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model_2=MultinomialNB()\n",
    "pipe_2 = Pipeline([('cleaner', CleanTextTransformer()),\n",
    "                 ('vectorizer', vectorizer),\n",
    "                 ('model',model_2)])\n",
    "\n",
    "\n",
    "pipe_2.fit(x_train,y_train)\n",
    "pred_2=pipe_2.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Yza37GcHCY2"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test,pred_2))\n",
    "print(confusion_matrix(y_test,pred_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a29I2Mo0HCbi"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model_3=LogisticRegression(multi_class='multinomial',solver='newton-cg')\n",
    "pipe_3 = Pipeline([('cleaner', CleanTextTransformer()),\n",
    "                 ('vectorizer', vectorizer),\n",
    "                 ('model',model_3)])\n",
    "\n",
    "\n",
    "pipe_3.fit(x_train,y_train)\n",
    "pred_3=pipe_3.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CvFJNCuyHCec"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test,pred_3))\n",
    "print(confusion_matrix(y_test,pred_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DpP_J28p2tze"
   },
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "filename='model.pkl'\n",
    "pickle.dump(pipe_1,open(filename,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wd2oZkjd2Hoi"
   },
   "outputs": [],
   "source": [
    "model=pickle.load(open('model.pkl','rb'))\n",
    "model.predict(x_test[13328])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sJBiOreh2HrS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMdKKf2pqvBY5OcjWK7p0HY",
   "collapsed_sections": [],
   "mount_file_id": "1fWt4jEPl1d2-pSbIWK9J1vFl_4gKgreO",
   "name": "Copy of Sentiment analysis.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
